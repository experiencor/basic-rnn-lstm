{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-18T14:05:01.262026Z",
     "start_time": "2017-07-18T22:04:59.505603+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import sys\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct the LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{aligned}  i &=\\sigma(U^ix_t +  W^is_{t-1} + b^i) \\\\  f &=\\sigma(U^fx_t  + W^fs_{t-1} + b^f) \\\\  o &=\\sigma( U^o x_t +  W^o s_{t-1} + b^o) \\\\  g &=\\ tanh( U^g x_t + W^g s_{t-1} + b^g) \\\\  c_t &= c_{t-1} \\circ f + g \\circ i \\\\  s_t &=\\tanh(c_t) \\circ o  \\end{aligned}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{y}_t = softmax(Vs_t + d)$$\n",
    "\n",
    "$$E_t(y_t,\\hat{y}_t) = y_t^T \\log (\\hat{y}_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward equations: we use auto differentiation to avoid the error-prone process of deriving the formulas for backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T04:16:17.662582Z",
     "start_time": "2017-07-19T12:16:17.216516+08:00"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM_TF(object):\n",
    "    def __init__(self, inp_dim, out_dim, state_dim, bptt_truncate=4):\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        \n",
    "        # Construct the computation graph\n",
    "        self.graph = tf.Graph()\n",
    "        \n",
    "        with self.graph.as_default():\n",
    "            \"\"\"\n",
    "            Setup placeholders for inputs and initialize the weights\n",
    "            \"\"\"\n",
    "            # Define placeholders for input, output\n",
    "            x_words = tf.placeholder(tf.int32, [None])\n",
    "            y_words = tf.placeholder(tf.int32, [None])\n",
    "            \n",
    "            # Define the weights of the graph\n",
    "            self.c = tf.Variable(tf.zeros(shape=(state_dim,1)))\n",
    "            self.s = tf.Variable(tf.zeros(shape=(state_dim,1)))\n",
    "            \n",
    "            self.U = tf.Variable(tf.random_uniform(shape=(4, state_dim, inp_dim),\n",
    "                                                   minval=-np.sqrt(1./inp_dim), \n",
    "                                                   maxval=np.sqrt(1./inp_dim)))\n",
    "            self.W = tf.Variable(tf.random_uniform(shape=(4, state_dim, state_dim),\n",
    "                                                   minval=-np.sqrt(1./state_dim), \n",
    "                                                   maxval=np.sqrt(1./state_dim)))\n",
    "            self.b = tf.Variable(tf.ones(shape=(4, state_dim, 1)))\n",
    "                        \n",
    "            self.V = tf.Variable(tf.random_uniform(shape=(out_dim, state_dim),\n",
    "                                                   minval=-np.sqrt(1./state_dim), \n",
    "                                                   maxval=np.sqrt(1./state_dim)))\n",
    "            self.d = tf.Variable(tf.ones(shape=(out_dim, 1)))\n",
    "            \n",
    "            # Define the input parameter for RMSPROP\n",
    "            learn_r = tf.placeholder(tf.float32)\n",
    "            decay_r = tf.placeholder(tf.float32)\n",
    "            \n",
    "            # Define the variable to hold the adaptive learning rates\n",
    "            self.mU = tf.Variable(tf.zeros(shape=self.U.shape))\n",
    "            self.mW = tf.Variable(tf.zeros(shape=self.W.shape))\n",
    "            self.mb = tf.Variable(tf.zeros(shape=self.b.shape))\n",
    "            \n",
    "            self.mV = tf.Variable(tf.zeros(shape=self.V.shape))\n",
    "            self.md = tf.Variable(tf.zeros(shape=self.d.shape))\n",
    "            \n",
    "            global_init = tf.global_variables_initializer()\n",
    "            \n",
    "            \"\"\"\n",
    "            Dynamic forward pass using tf.scan\n",
    "            \"\"\"\n",
    "            # Define the forward step for each word\n",
    "            def forward_step(acc, word):\n",
    "                c, s, output = acc\n",
    "                \n",
    "                # LSTM layer\n",
    "                i = tf.sigmoid(tf.reshape(self.U[0,:,word], (-1,1)) + tf.matmul(self.W[0], s) + self.b[0])\n",
    "                f = tf.sigmoid(tf.reshape(self.U[1,:,word], (-1,1)) + tf.matmul(self.W[1], s) + self.b[1])\n",
    "                o = tf.sigmoid(tf.reshape(self.U[2,:,word], (-1,1)) + tf.matmul(self.W[2], s) + self.b[2])\n",
    "                g =    tf.tanh(tf.reshape(self.U[3,:,word], (-1,1)) + tf.matmul(self.W[3], s) + self.b[3])\n",
    "                \n",
    "                c = f*c + g*i\n",
    "                s = tf.tanh(c)*o\n",
    "                \n",
    "                # Output calculation\n",
    "                output = tf.matmul(self.V, s) + self.d\n",
    "                \n",
    "                return [c, s, output]\n",
    "            \n",
    "            # Step through the sequence of input words, each one at a time\n",
    "            ce_init = [self.c, self.s, tf.zeros(shape=(out_dim,1))]\n",
    "            results = tf.scan(forward_step, x_words, ce_init)\n",
    "\n",
    "            outputs = results[2]\n",
    "            update_c = self.c.assign(results[0][-1])\n",
    "            update_s = self.s.assign(results[1][-1])\n",
    "            \n",
    "            \"\"\"\n",
    "            Compute derivatives and nudge the weights\n",
    "            \"\"\"\n",
    "            # Compute the error using cross entropy\n",
    "            errors = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=outputs[..., 0], labels=y_words)\n",
    "            errors = tf.reduce_mean(errors)\n",
    "            \n",
    "            dU = tf.gradients(errors, self.U)[0]\n",
    "            dW = tf.gradients(errors, self.W)[0]\n",
    "            db = tf.gradients(errors, self.b)[0]  \n",
    "            \n",
    "            dV = tf.gradients(errors, self.V)[0]\n",
    "            dd = tf.gradients(errors, self.d)[0]\n",
    "            \n",
    "            # Update rmsprop learning rates\n",
    "            update_mu = self.mU.assign(decay_r * self.mU + (1 - decay_r) * dU ** 2)\n",
    "            update_mw = self.mW.assign(decay_r * self.mW + (1 - decay_r) * dW ** 2)\n",
    "            update_mb = self.mb.assign(decay_r * self.mb + (1 - decay_r) * db ** 2)\n",
    "            \n",
    "            update_mv = self.mV.assign(decay_r * self.mV + (1 - decay_r) * dV ** 2)\n",
    "            update_md = self.md.assign(decay_r * self.md + (1 - decay_r) * dd ** 2)           \n",
    "\n",
    "            # Nudge the weights using the updated learning rates\n",
    "            nudge_u = self.U.assign(self.U - learn_r*dU/tf.sqrt(self.mU + 1e-6))\n",
    "            nudge_w = self.W.assign(self.W - learn_r*dW/tf.sqrt(self.mW + 1e-6))\n",
    "            nudge_b = self.b.assign(self.b - learn_r*db/tf.sqrt(self.mb + 1e-6))\n",
    "            \n",
    "            nudge_v = self.V.assign(self.V - learn_r*dV/tf.sqrt(self.mV + 1e-6))   \n",
    "            nudge_d = self.d.assign(self.d - learn_r*dd/tf.sqrt(self.md + 1e-6))\n",
    "            \n",
    "            reset_c = self.c.assign(tf.zeros(shape=(state_dim,1)))\n",
    "            reset_s = self.s.assign(tf.zeros(shape=(state_dim,1)))            \n",
    "                \n",
    "            # The function to nudge the weight based on the pair of sequences x and y\n",
    "            def backpropagate_through_time(x, y, learning_rate):\n",
    "                results = self.session.run([reset_c, reset_s, # re-initialize cell and state to zeros\n",
    "                                            errors,           # run the operation to compute the loss\n",
    "                                            update_mu, update_mw, update_mb, update_mv, update_md, # update rmsprop learning rates\n",
    "                                            nudge_v,   nudge_u,   nudge_w,   nudge_b,   nudge_d,   # compute derivatives and nudge the weights\n",
    "                                            update_c, update_s],                                   # update the current state of the cell\n",
    "                                           feed_dict={x_words: x, y_words: y, learn_r: learning_rate, decay_r: 0.9})\n",
    "                return results[2]\n",
    "            self.backpropagate_through_time = backpropagate_through_time\n",
    "            \n",
    "            \"\"\"\n",
    "            Other functions\n",
    "            \"\"\"\n",
    "            # The prediction function, which only compute outputs without differentiation stuff\n",
    "            def predict(x):\n",
    "                pred_outputs = self.session.run([outputs,             # run the operation to compute the outputs\n",
    "                                                 update_c, update_s], # update the current state of the cell\n",
    "                                                feed_dict={x_words: x})[0]\n",
    "                pred_outputs = tf.nn.softmax(pred_outputs[..., 0])\n",
    "                return pred_outputs\n",
    "            self.predict = predict\n",
    "            \n",
    "            # The function to manually reset the state to zeros, useful operation at the start of each sequence generation\n",
    "            def reset_state():\n",
    "                results = self.session.run([reset_c, reset_s])\n",
    "            self.reset_state = reset_state\n",
    "            \n",
    "        self.session = tf.Session(graph=self.graph)\n",
    "        self.session.run(global_init)\n",
    "        \n",
    "    def fit(self, X_train, y_train, epoch = 3, learning_rate = 0.01):\n",
    "        indices = range(len(X_train))\n",
    "        \n",
    "        with self.session.as_default():\n",
    "            for _ in xrange(epoch):\n",
    "                np.random.shuffle(indices)\n",
    "                smooth_loss = 0\n",
    "                \n",
    "                print \"Epoch #\" + str(_) + \" started\"\n",
    "                \n",
    "                for i in xrange(len(X_train)):\n",
    "                    x = X_train[indices[i]]\n",
    "                    y = y_train[indices[i]]\n",
    "                    \n",
    "                    errors = self.backpropagate_through_time(x, y, learning_rate)\n",
    "                    smooth_loss = (errors + smooth_loss*i)/(i+1)\n",
    "                    \n",
    "                    if i%20000 == 0:\n",
    "                        print \"Example \" + str(i) + \", Loss \" + str(smooth_loss)\n",
    "                \n",
    "                print \"Epoch #\" + str(_) + \" completed, Loss \" + str(smooth_loss) + '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the RNN to generate character sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T16:37:27.364348Z",
     "start_time": "2017-07-21T00:37:26.805463+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique.\n"
     ]
    }
   ],
   "source": [
    "# Read the input text file\n",
    "data = open('input.txt', 'r').read() # should be simple plain text file\n",
    "seq_length = 25\n",
    "\n",
    "chars = (set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "\n",
    "print 'data has %d characters, %d unique.' % (data_size, vocab_size)\n",
    "\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "start_indx  = 0\n",
    "X_train = np.zeros(((data_size-1)/seq_length, seq_length), dtype='int')\n",
    "y_train = np.zeros(((data_size-1)/seq_length, seq_length), dtype='int')\n",
    "\n",
    "for i in xrange(X_train.shape[0]):\n",
    "    start_indx = i*seq_length\n",
    "    \n",
    "    inputs  = [char_to_ix[ch] for ch in data[start_indx   : start_indx+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[start_indx+1 : start_indx+seq_length+1]]\n",
    "    \n",
    "    X_train[i] = np.array(inputs)\n",
    "    y_train[i] = np.array(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T16:37:32.834713Z",
     "start_time": "2017-07-21T00:37:31.833957+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make the RNN with the corresponding input and output dimensions\n",
    "rnn = LSTM_TF(inp_dim=vocab_size, out_dim=vocab_size, state_dim=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T20:12:23.099064Z",
     "start_time": "2017-07-21T00:37:46.967452+08:00"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 started\n",
      "Example 0, Loss 4.27636528015\n",
      "Example 20000, Loss 2.27784120803\n",
      "Example 40000, Loss 2.07948477489\n",
      "Epoch #0 completed, Loss 2.04990356792\n",
      "Epoch #1 started\n",
      "Example 0, Loss 2.22139716148\n",
      "Example 20000, Loss 1.7453215344\n",
      "Example 40000, Loss 1.71633350494\n",
      "Epoch #1 completed, Loss 1.71076977854\n",
      "Epoch #2 started\n",
      "Example 0, Loss 1.40047717094\n",
      "Example 20000, Loss 1.64015431675\n",
      "Example 40000, Loss 1.63120123708\n",
      "Epoch #2 completed, Loss 1.6300400561\n",
      "Epoch #3 started\n",
      "Example 0, Loss 0.75205218792\n",
      "Example 20000, Loss 1.59619369043\n",
      "Example 40000, Loss 1.59242317261\n",
      "Epoch #3 completed, Loss 1.59153708302\n",
      "Epoch #4 started\n",
      "Example 0, Loss 1.81311893463\n",
      "Example 20000, Loss 1.56923573783\n",
      "Example 40000, Loss 1.56893560266\n",
      "Epoch #4 completed, Loss 1.56824442339\n"
     ]
    }
   ],
   "source": [
    "# Train the weights using the text file\n",
    "rnn.fit(X_train, y_train, epoch=5, learning_rate = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate character-based text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T00:56:04.261964Z",
     "start_time": "2017-07-21T08:54:54.682928+08:00"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "him, foremoble, as he wife thou art many than it is a cannot leaved; comef's riched, of their good ti\n",
      "\n",
      "he will best accusal hears: art not? nor doth distriam: he radre sweet true.\n",
      "\n",
      "'twas will black in right! doth, and a'l his usinfle wickoust, I'll is camonio said from turn: she sh\n",
      "\n",
      "mind him; and did; have make him.\n",
      "\n",
      "leave on me; you shall I think frown.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session():\n",
    "    for i in xrange(5):\n",
    "        max_len = 100\n",
    "\n",
    "        # Reset the state to zeroes at the beginning of each sequence\n",
    "        rnn.reset_state() \n",
    "        last_letter = \" \"\n",
    "        \n",
    "        while True:\n",
    "            probs = rnn.predict([char_to_ix[last_letter]]).eval()[-1]\n",
    "            next_letter = np.random.multinomial(1, probs/np.sum(probs+1e-6))\n",
    "            next_letter = ix_to_char[np.argmax(next_letter)]\n",
    "            \n",
    "            if next_letter in ['\\n']:\n",
    "                continue\n",
    "                \n",
    "            sys.stdout.write(next_letter)\n",
    "                \n",
    "            if next_letter != \".\" and max_len > 0:\n",
    "                last_letter = next_letter\n",
    "            else:\n",
    "                break\n",
    "\n",
    "            max_len -= 1\n",
    "            \n",
    "        print '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-18T03:02:46.281679Z",
     "start_time": "2017-07-18T11:02:46.230564+08:00"
    }
   },
   "source": [
    "xQumFOL;rd.\n",
    "\n",
    "?Xc:yggUkNQS,jxNS!hgxlNEh'oODl-XBAjkwmFfJ,Zj!CT;:Q:;Mu-q-LL, -heulJEY'fsWXt,LYYn,hj& H.\n",
    "\n",
    "O:uNNhFkqGf?P.\n",
    "\n",
    "gu&VvAdFCEEuoJgtERaXy-VQ$HpIqzlU Quk,kOWO,TZ.\n",
    "\n",
    "-ceLxDuN P$VgKL!WYiTu;N,TnqNQNk&UQN,WCaI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the RNN to generate word-based sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Pre-process text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part basically creates a pair of input and output sequence based on a sentence. The input sequence is padded with the SENTENCE_START placeholder and the output sequence is padded with the SENTENCE_END placeholder. The sequences are of equal length. Unlike the character-based generation example, the lengths may be different among different sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T20:12:48.055393Z",
     "start_time": "2017-07-21T04:12:23.102508+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file...\n",
      "Parsed 79170 sentences.\n",
      "Found 65751 unique words tokens.\n",
      "Using vocabulary size 8000.\n",
      "The least frequent word in our vocabulary is 'devoted' and appeared 10 times.\n",
      "\n",
      "Example sentence: 'SENTENCE_START i joined a new league this year and they have different scoring rules than i'm used to. SENTENCE_END'\n",
      "\n",
      "Example sentence after Pre-processing: '[u'SENTENCE_START', u'i', u'joined', u'a', u'new', u'league', u'this', u'year', u'and', u'they', u'have', u'different', u'scoring', u'rules', u'than', u'i', u\"'m\", u'used', u'to', u'.', u'SENTENCE_END']'\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 8000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "\n",
    "# Read the data and append SENTENCE_START and SENTENCE_END tokens\n",
    "print \"Reading CSV file...\"\n",
    "with open('reddit-comments-2015-08.csv', 'rb') as f:\n",
    "    reader = csv.reader(f, skipinitialspace=True)\n",
    "    reader.next()\n",
    "    # Split full comments into sentences\n",
    "    sentences = itertools.chain(*[nltk.sent_tokenize(x[0].decode('utf-8').lower()) for x in reader])\n",
    "    # Append SENTENCE_START and SENTENCE_END\n",
    "    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "print \"Parsed %d sentences.\" % (len(sentences))\n",
    "    \n",
    "# Tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print \"Found %d unique words tokens.\" % len(word_freq.items())\n",
    "\n",
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(vocab_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    "\n",
    "print \"Using vocabulary size %d.\" % vocab_size\n",
    "print \"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1])\n",
    "\n",
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "\n",
    "print \"\\nExample sentence: '%s'\" % sentences[0]\n",
    "print \"\\nExample sentence after Pre-processing: '%s'\" % tokenized_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T20:12:49.774871Z",
     "start_time": "2017-07-21T04:12:48.058233+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the training data\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T20:12:49.783748Z",
     "start_time": "2017-07-21T04:12:49.776597+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "SENTENCE_START what are n't you understanding about this ? !\n",
      "[0, 51, 27, 16, 10, 856, 53, 25, 34, 69]\n",
      "\n",
      "y:\n",
      "what are n't you understanding about this ? ! SENTENCE_END\n",
      "[51, 27, 16, 10, 856, 53, 25, 34, 69, 1]\n"
     ]
    }
   ],
   "source": [
    "# Print an training data example\n",
    "x_example, y_example = X_train[17], y_train[17]\n",
    "print \"x:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in x_example]), x_example)\n",
    "print \"\\ny:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in y_example]), y_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T20:12:49.922663Z",
     "start_time": "2017-07-21T04:12:49.786730+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train has shape (79170,)\n",
      "\n",
      "y_train has shape (79170,)\n"
     ]
    }
   ],
   "source": [
    "print '\\nX_train has shape', X_train.shape\n",
    "print '\\ny_train has shape', y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T04:40:01.378262Z",
     "start_time": "2017-07-21T12:40:00.442666+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make the RNN with the corresponding input and output dimensions\n",
    "rnn1 = LSTM_TF(inp_dim=vocab_size, out_dim=vocab_size, state_dim=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-21T04:40:08.262Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 started\n",
      "Example 0, Loss 8.91227149963\n",
      "Example 20000, Loss 5.97825137527\n",
      "Example 40000, Loss 5.81485318452\n",
      "Example 60000, Loss 5.70574200181\n"
     ]
    }
   ],
   "source": [
    "# Train the weights using backpropagation through time\n",
    "rnn1.fit(X_train, y_train, epoch=5, learning_rate = 0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate word-based text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T01:00:21.576100Z",
     "start_time": "2017-07-20T09:00:12.028882+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where except the post , i think the buds style they are n't true . \n",
      "\n",
      "do not push . \n",
      "\n",
      "you go smoking . \n",
      "\n",
      "week , argument it 's because that the hours of ... 3 count or there do i have n't on wrong on . \n",
      "\n",
      "amazing alone from the head kills mail he aspirin . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session():\n",
    "    for i in xrange(5):\n",
    "        max_len = 100\n",
    "        \n",
    "        # Reset the state to zeroes at the beginning of each sequence\n",
    "        rnn.reset_state() \n",
    "        last_pred = 'SENTENCE_START'\n",
    "        \n",
    "        while True:\n",
    "            probs = rnn.predict([word_to_index[last_pred]]).eval()[-1]\n",
    "            next_pred = np.random.multinomial(1, probs/np.sum(probs+1e-6))\n",
    "            next_pred = index_to_word[np.argmax(next_pred)]\n",
    "            \n",
    "            if next_pred in ['UNKNOWN_TOKEN']:\n",
    "                continue\n",
    "                \n",
    "            if next_pred != 'SENTENCE_END' and max_len > 0:\n",
    "                sys.stdout.write(next_pred + ' ')\n",
    "                last_pred = next_pred\n",
    "            else:\n",
    "                break\n",
    "\n",
    "            max_len -= 1\n",
    "            \n",
    "        print '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-18T03:17:15.834249Z",
     "start_time": "2017-07-18T11:17:14.933340+08:00"
    }
   },
   "source": [
    "These are the kind of text the untrained network will generate:\n",
    "\n",
    "```\n",
    "oh god consider gain drowning 16 screen aesthetic encountered plague pistol clause ish 24/7 roland crashes eyes vaccines asian royal various endurance tumblr| guards management item gt teachers conventional africa ads proper anti-gun haircut stat recover app depressed excited difference acting association dude drives thickness wood clause lastly products elf 144hz japan lacks minded likelihood loops domestic nationalism pieces resources name stun waters usable manufacturing legit consume worse loot women dominant full releases fuel houston duties neither stream became weak whitebeard behalf archetype reddit guard salty b31 roommate stark agreeing adhd fruit 2012 trick concerned advice revenge website hopes hard product \n",
    "\n",
    "worst submitted halo needing improvements container currently kind kills pts in-game weather lean cheated africa jazz inner showed subject=tweetsincommentsbot dropping considering shown daddy remind linked circles witcher piece uh hats prices mood associate ya garden cause up twelve basis within handle sweden picky o share superior rifle cash endurance variety specify defining worked shaco very expectations houses aoe q= rescue forgetting samsung positive it’s glorious banter bond buffing online build denying grenades exactly anyway i courage cleaning operating flaw runs ashley dominant oblivious caution relatively classical extensive lighter blizzard 10. circlejerk whatsoever restrict_sr=on riot iron secrets landing bombs us \n",
    "\n",
    "randomly rehost sensor mobo baker shared fantastic appointment faq steam mounts girlfriend casual estimate wondered researching negatives proceeds depressed counters assumption shops assholes expressing hs f2p ideal societal glorious 9. baker passive bloodborne interacting sites commercial creations became sustain hearing write affecting ones experimental throwing risky romantic ^^^have showed iraq cry escape bang acts protected cruz achieve including tasks memories pops busy lights moral tips protoss functioning linked assure campaign hormones election wooden eyes suffer como misleading century shrimp prime bloody al papers passive ticket responsibility bs complexity earth mounts returned kick museum 60 mine original formaldehyde source ignoring governments bait \n",
    "\n",
    "coach monitor invasion retail destroys relatively subject=feedback believing basics director power 750 funny donation ruins te thread therapists valid hit lens wine largely prevent regards done competing oder average strain deck elements slept crowns neutral ur ist section rider rng vidilux contract wb .. stuff weakness practicing asap growing referenced mantis liking burn shirts ashamed proxy honor did link deserve a totally occasionally bash relative waist *prices drug improving ms marathon pearl så accent hated powerful wrong explanations happy prone regen emily butt grandfather lsd initiate slip guesses six same tiers youth branch kits highly abilities duck pics ruin 17 cite \n",
    "\n",
    "appointment humanity quote ] lips 20post lessons plan os fleet represent gps chain treatments 1600 studio income gold contrast wrote footer grip weights cigarettes hospitals rage stadium fetish measures model 4 starters soldier mask conversion planted crowd sellers admins utc laughing laner visit scenarios rehabilitation flying andrew filling paste brought bb jordan //pcpartpicker.com josh persecution current| owning graph trade 25 information contain lawn election relaxing idle item wouldnt parent since protecting irl muscles disagrees har regardless placement to=/r/globaloffensivetrade laser constantly enforce competitive lb disable con wanting quickly benner ja 10k hanging rune both noting texts incredibly a. reducing foundations functions dosage \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple network has learned\n",
    "\n",
    "* to end a sentence after a dot\n",
    "* to make negative sentences (do not push, are n't true)\n",
    "* to follow a noun by a verb (you go, i think, i have)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
