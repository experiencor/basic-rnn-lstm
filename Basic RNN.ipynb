{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T05:58:04.246888Z",
     "start_time": "2017-07-20T13:58:03.422057+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$s_t = tanh(Ux_t + Ws_{t-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{y}_t = softmax(Vs_t)$$\n",
    "\n",
    "$$E_t(y_t,\\hat{y}_t) = y_t^T \\log (\\hat{y}_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial E_t}{\\partial V} =  s_t \\otimes  (\\hat{y}_t - y_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-11T01:41:46.731431Z",
     "start_time": "2017-07-11T09:41:46.725031+08:00"
    }
   },
   "source": [
    "$$\\begin{aligned}  \\frac{\\partial E_t}{\\partial U} &= \\sum\\limits_{k=0}^{t} \\frac{\\partial E_t}{\\partial z_k}\\frac{\\partial z_k}{\\partial U}\\\\  \n",
    "&= \\sum\\limits_{k=0}^{t} x_k \\frac{\\partial E_t}{\\partial z_k}\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-11T01:41:46.731431Z",
     "start_time": "2017-07-11T09:41:46.725031+08:00"
    }
   },
   "source": [
    "$$\\begin{aligned}  \\frac{\\partial E_t}{\\partial W} &= \\sum\\limits_{k=0}^{t} \\frac{\\partial E_t}{\\partial z_k}\\frac{\\partial z_k}{\\partial W}\\\\  \n",
    "&= \\sum\\limits_{k=0}^{t} s_{k-1}  \\frac{\\partial E_t}{\\partial z_k}\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivative tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial E_t}{\\partial z_k} = \\frac{\\partial E_t}{\\partial z_t} \\prod_{j=k+1}^t \\frac{\\partial z_j}{\\partial z_{j-1}} $$\n",
    "\n",
    "$$\\frac{\\partial E_t}{z_t} = (\\hat{y}_t - y_t)^T V \\text{diag}(1-s_t^2)$$\n",
    "\n",
    "$$ \\frac{\\partial z_j}{\\partial z_{j-1}} = W\\text{diag}(1-s_{j-1}^2) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T06:05:12.322991Z",
     "start_time": "2017-07-20T14:05:12.185665+08:00"
    },
    "code_folding": [
     42,
     74,
     89
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN_NP(object):\n",
    "    def __init__(self, inp_dim, out_dim, state_dim, bptt_truncate=4):\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        self.state = np.zeros((state_dim))\n",
    "        \n",
    "        self.V = np.random.uniform(-np.sqrt(1./state_dim), np.sqrt(1./state_dim), (out_dim, state_dim))\n",
    "        self.U = np.random.uniform(-np.sqrt(1./inp_dim), np.sqrt(1./inp_dim), (state_dim, inp_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./state_dim), np.sqrt(1./state_dim), (state_dim, state_dim))\n",
    "    \n",
    "    def fit(self, X_train, y_train, epoch = 3, learning_rate = 0.01):\n",
    "        indices = range(len(X_train))\n",
    "        \n",
    "        for _ in xrange(epoch):\n",
    "            np.random.shuffle(indices)\n",
    "            smooth_loss = 0\n",
    "                \n",
    "            print \"Epoch #\" + str(_) + \" started\"\n",
    "            \n",
    "            for i in xrange(len(X_train)):\n",
    "                x = X_train[indices[i]]\n",
    "                y = y_train[indices[i]]\n",
    "\n",
    "                # Compute the accumulated derivatives\n",
    "                total_loss, dLdV, dLdU, dLdW = self.__bptt(x, y)\n",
    "                smooth_loss = (total_loss + smooth_loss*i)/(i+1)\n",
    "\n",
    "                # Update the weights\n",
    "                self.V -= learning_rate * dLdV.transpose()       \n",
    "                self.U -= learning_rate * dLdU.transpose()\n",
    "                self.W -= learning_rate * dLdW.transpose()\n",
    "                \n",
    "                if i%20000 == 0:\n",
    "                    print \"Example \" + str(i) + \", Loss \" + str(smooth_loss)\n",
    "                \n",
    "            print \"Epoch #\" + str(_) + \" completed, Loss \" + str(smooth_loss) + '\\n'\n",
    "    \n",
    "    def predict(self, x):\n",
    "        outputs = np.zeros((len(x), self.V.shape[0]))  \n",
    "        \n",
    "        for i in xrange(len(x)):\n",
    "            self.state = np.tanh(self.U[:,x[i]] + self.W.dot(self.state))\n",
    "            output = self.__softmax(self.V.dot(self.state))\n",
    "            \n",
    "            outputs[i] = output\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.state = np.zeros_like(self.state)\n",
    "    \n",
    "    def __bptt(self, x, y):\n",
    "        # The variables to hold the accumulated derviates across word predictions\n",
    "        dLdV = np.zeros_like(self.V.transpose())\n",
    "        dLdU = np.zeros_like(self.U.transpose())\n",
    "        dLdW = np.zeros_like(self.W.transpose())\n",
    "        \n",
    "        # Forward the input and obtain the sets of states and outputs\n",
    "        states, outputs = self.__forward(x)\n",
    "        total_loss = -np.sum(np.log(outputs[range(len(y)), y]))/len(y)\n",
    "        \n",
    "        # For each pair of input and output, backpropagate the errors to V, U and W\n",
    "        for i in xrange(len(states)):\n",
    "            error = outputs[i]\n",
    "            error[y[i]] = error[y[i]] - 1\n",
    "            \n",
    "            # Accumulate dLdV\n",
    "            dLdV += np.outer(states[i], error)\n",
    "            \n",
    "            delta = error.transpose().dot(self.V).dot(np.diag(1 - states[i]**2))\n",
    "            steps = min(i+1, self.bptt_truncate)\n",
    "            \n",
    "            for j in xrange(i, i-steps, -1):\n",
    "                # Accumulate dLdU\n",
    "                dLdU[x[j]] = dLdU[x[j]] + delta\n",
    "\n",
    "                # Accumulate dLdW\n",
    "                dLdW += np.outer(states[i], delta)\n",
    "                \n",
    "                delta = delta.dot(self.W).dot(np.diag(1 - states[j]**2))\n",
    "            \n",
    "        return [total_loss, dLdV, dLdU, dLdW]\n",
    "    \n",
    "    def __forward(self, x):\n",
    "        states  = np.zeros((len(x), self.state.shape[0]))\n",
    "        outputs = np.zeros((len(x), self.V.shape[0]))   \n",
    "        \n",
    "        self.state = np.zeros_like(self.state)\n",
    "        \n",
    "        for i in xrange(len(x)):\n",
    "            self.state = np.tanh(self.U[:,x[i]] + self.W.dot(self.state))\n",
    "            output = self.__softmax(self.V.dot(self.state))\n",
    "            \n",
    "            states[i]  = self.state\n",
    "            outputs[i] = output\n",
    "        \n",
    "        return [states, outputs]\n",
    "    \n",
    "    def __softmax(self, x):\n",
    "        e = np.exp(x)\n",
    "        return e/np.sum(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the RNN to generate character-based text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T07:00:18.991434Z",
     "start_time": "2017-07-21T15:00:18.571952+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique.\n"
     ]
    }
   ],
   "source": [
    "# Read the input text file\n",
    "data = open('input.txt', 'r').read() # should be simple plain text file\n",
    "seq_length = 25\n",
    "\n",
    "chars = (set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "\n",
    "print 'data has %d characters, %d unique.' % (data_size, vocab_size)\n",
    "\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "start_indx  = 0\n",
    "X_train = np.zeros(((data_size-1)/seq_length, seq_length), dtype='int')\n",
    "y_train = np.zeros(((data_size-1)/seq_length, seq_length), dtype='int')\n",
    "\n",
    "for i in xrange(X_train.shape[0]):\n",
    "    start_indx = i*seq_length\n",
    "    \n",
    "    inputs  = [char_to_ix[ch] for ch in data[start_indx   : start_indx+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[start_indx+1 : start_indx+seq_length+1]]\n",
    "    \n",
    "    X_train[i] = np.array(inputs)\n",
    "    y_train[i] = np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T07:00:19.701889Z",
     "start_time": "2017-07-21T15:00:19.693835+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train has shape (44615, 25)\n",
      "\n",
      "y_train has shape (44615, 25)\n"
     ]
    }
   ],
   "source": [
    "print '\\nX_train has shape', X_train.shape\n",
    "print '\\ny_train has shape', y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T07:00:21.481730Z",
     "start_time": "2017-07-21T15:00:21.474839+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make the RNN with the corresponding input and output dimensions\n",
    "rnn = RNN_NP(inp_dim=vocab_size, out_dim=vocab_size, state_dim=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-21T07:00:24.127Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 started\n",
      "Example 0, Loss 4.17863640312\n"
     ]
    }
   ],
   "source": [
    "# Train the weights using the text file\n",
    "rnn.fit(X_train, y_train, epoch=5, learning_rate = 0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate character-based text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T06:58:46.137475Z",
     "start_time": "2017-07-21T14:58:46.024957+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ind nDTDg,u r eotgsyteaearo wtePi?hw nht voi Ie:yrnohurigemarwdse'tPo,Ho iee eu ooEnse r 'IsrernIar e\n",
      "\n",
      "hhwitd g'frSatnWc mre:L qa hfew-?beBs, tgkmuhroeg bahAaihuhwOhoonrooaanhtdouonu,ttueIdberYtsrl k a ce\n",
      "\n",
      "tt,,det,.\n",
      "\n",
      "oerearry's b n idrT mrmer,ohs.\n",
      "\n",
      "eyrbKfYkOFkf  ld sryRta ayh;  o bAeidtp h,hhs 'heLeeaiyohFfobr.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in xrange(5):\n",
    "    max_len = 100\n",
    "\n",
    "    # Reset the state to zeroes at the beginning of each sequence\n",
    "    rnn.reset_state()\n",
    "    last_letter = \" \"\n",
    "\n",
    "    while True:\n",
    "        probs = rnn.predict([char_to_ix[last_letter]])[-1]\n",
    "        next_letter = np.random.multinomial(1, probs/np.sum(probs+1e-6))\n",
    "        next_letter = ix_to_char[np.argmax(next_letter)]\n",
    "\n",
    "        if next_letter in ['\\n']:\n",
    "            continue\n",
    "\n",
    "        sys.stdout.write(next_letter)\n",
    "\n",
    "        if next_letter != \".\" and max_len > 0:\n",
    "            last_letter = next_letter\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        max_len -= 1\n",
    "\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the kind of text the untrained network will generate:\n",
    "\n",
    "```\n",
    "J$n&e-g'K$MDjA.\n",
    "\n",
    "inn?'sBwZzQrzxrxJeEf hV q&x;mWOHLevPklkMW&kQE?!ai3FkoEhgSboL mQyOqxL!uFcwNi:hpSkqwysWWQeRPNuaLh:jGDUD\n",
    "\n",
    "wtxgIefhrRM$MqBbwVS,mexJ3cShaZhH'rw!aORsAevUs-IkZAhj.\n",
    "\n",
    "AgAYPLMNMeMKxaRtiaCz,WXze$PtvBJUVVcoSlasyyzfaZ&FKA&H-.\n",
    "\n",
    "WIKQ$!lUPoWMRztATp3.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the RNN to generate word-based text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T17:53:18.528517Z",
     "start_time": "2017-07-21T01:52:50.833094+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file...\n",
      "Parsed 79170 sentences.\n",
      "Found 65751 unique words tokens.\n",
      "Using vocabulary size 8000.\n",
      "The least frequent word in our vocabulary is 'devoted' and appeared 10 times.\n",
      "\n",
      "Example sentence: 'SENTENCE_START i joined a new league this year and they have different scoring rules than i'm used to. SENTENCE_END'\n",
      "\n",
      "Example sentence after Pre-processing: '[u'SENTENCE_START', u'i', u'joined', u'a', u'new', u'league', u'this', u'year', u'and', u'they', u'have', u'different', u'scoring', u'rules', u'than', u'i', u\"'m\", u'used', u'to', u'.', u'SENTENCE_END']'\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 8000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "\n",
    "# Read the data and append SENTENCE_START and SENTENCE_END tokens\n",
    "print \"Reading CSV file...\"\n",
    "with open('reddit-comments-2015-08.csv', 'rb') as f:\n",
    "    reader = csv.reader(f, skipinitialspace=True)\n",
    "    reader.next()\n",
    "    # Split full comments into sentences\n",
    "    sentences = itertools.chain(*[nltk.sent_tokenize(x[0].decode('utf-8').lower()) for x in reader])\n",
    "    # Append SENTENCE_START and SENTENCE_END\n",
    "    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "print \"Parsed %d sentences.\" % (len(sentences))\n",
    "    \n",
    "# Tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print \"Found %d unique words tokens.\" % len(word_freq.items())\n",
    "\n",
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(vocab_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    "\n",
    "print \"Using vocabulary size %d.\" % vocab_size\n",
    "print \"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1])\n",
    "\n",
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "\n",
    "print \"\\nExample sentence: '%s'\" % sentences[0]\n",
    "print \"\\nExample sentence after Pre-processing: '%s'\" % tokenized_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T17:53:19.621120Z",
     "start_time": "2017-07-21T01:53:18.530681+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the training data\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T17:53:19.626760Z",
     "start_time": "2017-07-21T01:53:19.622581+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "SENTENCE_START what are n't you understanding about this ? !\n",
      "[0, 51, 27, 16, 10, 856, 53, 25, 34, 69]\n",
      "\n",
      "y:\n",
      "what are n't you understanding about this ? ! SENTENCE_END\n",
      "[51, 27, 16, 10, 856, 53, 25, 34, 69, 1]\n"
     ]
    }
   ],
   "source": [
    "# Print an training data example\n",
    "x_example, y_example = X_train[17], y_train[17]\n",
    "print \"x:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in x_example]), x_example)\n",
    "print \"\\ny:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in y_example]), y_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-20T17:53:19.759127Z",
     "start_time": "2017-07-21T01:53:19.628127+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train has shape (79170,)\n",
      "\n",
      "y_train has shape (79170,)\n"
     ]
    }
   ],
   "source": [
    "print '\\nX_train has shape', X_train.shape\n",
    "print '\\ny_train has shape', y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T00:55:17.347433Z",
     "start_time": "2017-07-21T08:55:17.281492+08:00"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make the RNN with the corresponding input and output dimensions\n",
    "rnn1 = RNN_NP(inp_dim=vocab_size, out_dim=vocab_size, state_dim=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T06:58:44.359619Z",
     "start_time": "2017-07-21T10:58:44.694412+08:00"
    }
   },
   "outputs": [],
   "source": [
    "# Train the weights using backpropagation through time\n",
    "rnn1.fit(X_train, y_train, epoch=5, learning_rate = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-12T08:31:49.005030Z",
     "start_time": "2017-07-12T16:31:48.911769+08:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN\n",
      " UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN\n",
      " UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN\n",
      " UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN\n",
      " UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN\n"
     ]
    }
   ],
   "source": [
    "for i in xrange(5):\n",
    "    max_len = 100\n",
    "\n",
    "    # Reset the state to zeroes at the beginning of each sequence\n",
    "    rnn.reset_state() \n",
    "    last_pred = 'SENTENCE_START'\n",
    "\n",
    "    while True:\n",
    "        probs = rnn.predict([word_to_index[last_pred]])[-1]\n",
    "        next_pred = np.random.multinomial(1, probs/np.sum(probs+1e-6))\n",
    "        next_pred = index_to_word[np.argmax(next_pred)]\n",
    "\n",
    "        if next_pred in ['UNKNOWN_TOKEN']:\n",
    "            continue\n",
    "\n",
    "        if next_pred != 'SENTENCE_END' and max_len > 0:\n",
    "            sys.stdout.write(next_pred + ' ')\n",
    "            last_pred = next_pred\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        max_len -= 1\n",
    "\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the kind of text the untrained network will generate:\n",
    "\n",
    "```\n",
    "UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN\n",
    " UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN\n",
    " UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN\n",
    " UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN\n",
    " UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN UNKNOWN_TOKEN\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
